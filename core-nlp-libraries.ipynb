{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Core NLP Libraries"
      ],
      "metadata": {
        "id": "wAGy4IUIAEiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK (Natural Language Toolkit)\n",
        "\n"
      ],
      "metadata": {
        "id": "BbeqDMgQ1bG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Python library designed to work with human language data\n",
        "- access to over 50 corpora and lexical resources\n",
        "- offers tools for fundamental NLP tasks:\n",
        "    1. Tokenization (splitting text into words)\n",
        "    2. Stemming (reducing words to their root form)\n",
        "    3. Tagging (assigning grammatical labels to the words role in the text, e.g. nouns, verbs, adjectives)\n",
        "    4. Parsing (analyzing sentence structure and aiming to identify and classify named entities, e.g. people, locations)\n",
        "    5. Classification\n",
        "    6. Semantic Reasoning\n",
        "- free, open-source and cross-platform compatibility (Windows, macOS, Linux)\n",
        "\n",
        "### Resources:\n",
        "- https://www.nltk.org/\n",
        "- https://www.nltk.org/book/\n",
        "- https://github.com/hb20007/hands-on-nltk-tutorial\n",
        "\n",
        "### ABC's of nltk"
      ],
      "metadata": {
        "id": "aDqyI4b6AJef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary NLTK data (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "cMBOEFhu3nWY",
        "outputId": "08352816-af40-4e84-8032-adc87222694b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate # For prettier output\n",
        "\n",
        "text = \"At the Broad Institute in Cambridge, Dr. Jennifer Doudna's work with CRISPR technology transformed genetic research.\"\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\n--- Tokenization ---\")\n",
        "print(tokens)\n",
        "\n",
        "# 2. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"\\n--- Stemming ---\")\n",
        "print(stemmed_tokens)\n",
        "\n",
        "# 3. Part-of-Speech Tagging\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "print(\"\\n--- Part-of-Speech Tagging ---\")\n",
        "print(tabulate(tagged_tokens, headers=[\"Token\", \"POS Tag\"], tablefmt=\"fancy_grid\"))\n",
        "\n",
        "# 4. Parsing (Named Entity Recognition)\n",
        "ner_tree = ne_chunk(tagged_tokens)\n",
        "\n",
        "print(\"\\n--- Named Entity Recognition (Tree Structure) ---\")\n",
        "ner_tree.pretty_print()\n",
        "\n",
        "print(\"\\n--- Named Entity Recognition (Tabular Representation) ---\")\n",
        "\n",
        "ner_results = []\n",
        "for subtree in ner_tree:\n",
        "    if hasattr(subtree, 'label'):  # Check if it's a named entity chunk\n",
        "        entity = \" \".join(word for word, tag in subtree.leaves())\n",
        "        ner_results.append((entity, subtree.label()))\n",
        "    else:  # It's a regular token\n",
        "        ner_results.append((subtree[0], 'O')) # 'O' means 'Outside' any named entity.\n",
        "\n",
        "print(tabulate(ner_results, headers=[\"Entity/Token\", \"NER Tag\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "e0h8dw_W3sN4",
        "outputId": "5b8f22c9-70fe-4e40-a731-ae21132cefe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tokenization ---\n",
            "['At', 'the', 'Broad', 'Institute', 'in', 'Cambridge', ',', 'Dr.', 'Jennifer', 'Doudna', \"'s\", 'work', 'with', 'CRISPR', 'technology', 'transformed', 'genetic', 'research', '.']\n",
            "\n",
            "--- Stemming ---\n",
            "['at', 'the', 'broad', 'institut', 'in', 'cambridg', ',', 'dr.', 'jennif', 'doudna', \"'s\", 'work', 'with', 'crispr', 'technolog', 'transform', 'genet', 'research', '.']\n",
            "\n",
            "--- Part-of-Speech Tagging ---\n",
            "╒═════════════╤═══════════╕\n",
            "│ Token       │ POS Tag   │\n",
            "╞═════════════╪═══════════╡\n",
            "│ At          │ IN        │\n",
            "├─────────────┼───────────┤\n",
            "│ the         │ DT        │\n",
            "├─────────────┼───────────┤\n",
            "│ Broad       │ NNP       │\n",
            "├─────────────┼───────────┤\n",
            "│ Institute   │ NNP       │\n",
            "├─────────────┼───────────┤\n",
            "│ in          │ IN        │\n",
            "├─────────────┼───────────┤\n",
            "│ Cambridge   │ NNP       │\n",
            "├─────────────┼───────────┤\n",
            "│ ,           │ ,         │\n",
            "├─────────────┼───────────┤\n",
            "│ Dr.         │ NNP       │\n",
            "├─────────────┼───────────┤\n",
            "│ Jennifer    │ NNP       │\n",
            "├─────────────┼───────────┤\n",
            "│ Doudna      │ NNP       │\n",
            "├─────────────┼───────────┤\n",
            "│ 's          │ POS       │\n",
            "├─────────────┼───────────┤\n",
            "│ work        │ NN        │\n",
            "├─────────────┼───────────┤\n",
            "│ with        │ IN        │\n",
            "├─────────────┼───────────┤\n",
            "│ CRISPR      │ NNP       │\n",
            "├─────────────┼───────────┤\n",
            "│ technology  │ NN        │\n",
            "├─────────────┼───────────┤\n",
            "│ transformed │ VBD       │\n",
            "├─────────────┼───────────┤\n",
            "│ genetic     │ JJ        │\n",
            "├─────────────┼───────────┤\n",
            "│ research    │ NN        │\n",
            "├─────────────┼───────────┤\n",
            "│ .           │ .         │\n",
            "╘═════════════╧═══════════╛\n",
            "\n",
            "--- Named Entity Recognition (Tree Structure) ---\n",
            "                                                                         S                                                                                                                            \n",
            "   ______________________________________________________________________|_____________________________________________________________________________________________________________________        \n",
            "  |     |      |    |     |         |         |       |       |          |              |            |           |       |            ORGANIZATION                    GPE        PERSON   ORGANIZATION\n",
            "  |     |      |    |     |         |         |       |       |          |              |            |           |       |       __________|_____________              |           |           |       \n",
            "At/IN the/DT in/IN ,/, Dr./NNP Jennifer/NNP 's/POS work/NN with/IN technology/NN transformed/VBD genetic/JJ research/NN ./. Broad/NNP              Institute/NNP Cambridge/NNP Doudna/NNP  CRISPR/NNP \n",
            "\n",
            "\n",
            "--- Named Entity Recognition (Tabular Representation) ---\n",
            "╒═════════════════╤══════════════╕\n",
            "│ Entity/Token    │ NER Tag      │\n",
            "╞═════════════════╪══════════════╡\n",
            "│ At              │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ the             │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ Broad Institute │ ORGANIZATION │\n",
            "├─────────────────┼──────────────┤\n",
            "│ in              │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ Cambridge       │ GPE          │\n",
            "├─────────────────┼──────────────┤\n",
            "│ ,               │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ Dr.             │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ Jennifer        │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ Doudna          │ PERSON       │\n",
            "├─────────────────┼──────────────┤\n",
            "│ 's              │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ work            │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ with            │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ CRISPR          │ ORGANIZATION │\n",
            "├─────────────────┼──────────────┤\n",
            "│ technology      │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ transformed     │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ genetic         │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ research        │ O            │\n",
            "├─────────────────┼──────────────┤\n",
            "│ .               │ O            │\n",
            "╘═════════════════╧══════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Classification Example (Simple Sentiment Analysis using Naive Bayes)\n",
        "def simple_sentiment_analysis(text):\n",
        "    def word_feats(words):\n",
        "        return dict([(word, True) for word in words])\n",
        "\n",
        "    positive_words = ['good', 'awesome', 'fantastic', 'amazing']\n",
        "    negative_words = ['bad', 'terrible', 'awful', 'horrible']\n",
        "\n",
        "    positive_features = [(word_feats(positive_words), 'pos')]\n",
        "    negative_features = [(word_feats(negative_words), 'neg')]\n",
        "\n",
        "    train_set = positive_features + negative_features\n",
        "    classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "    words = text.split()\n",
        "    feats = word_feats(words)\n",
        "    return classifier.classify(feats)\n",
        "\n",
        "print(simple_sentiment_analysis(\"I really liked the play, the actors were amazing!\"))\n",
        "print(simple_sentiment_analysis(\"This play was so sad. I felt awful afterwards.\"))\n",
        "print(simple_sentiment_analysis(\"This play was so sad. I felt awful afterwards. The acting was fantastic and the sound quality good.\"))\n",
        "\n",
        "# Semantic Reasoning Example (Using WordNet)\n",
        "def simple_semantic_reasoning(word1, word2):\n",
        "    synsets1 = wordnet.synsets(word1)\n",
        "    synsets2 = wordnet.synsets(word2)\n",
        "\n",
        "    if synsets1 and synsets2:\n",
        "        similarity = synsets1[0].wup_similarity(synsets2[0]) # Wu-Palmer similarity\n",
        "        if similarity is not None and similarity > 0.5: # arbitrary threshold\n",
        "            return f\"'{word1}' and '{word2}' are semantically similar (similarity: {similarity:.2f})\"\n",
        "        else:\n",
        "            return f\"'{word1}' and '{word2}' are not very similar (similarity: {similarity:.2f})\"\n",
        "\n",
        "    else:\n",
        "        return \"One or both words not found in WordNet.\"\n",
        "\n",
        "print(simple_semantic_reasoning(\"house\", \"apartment\")) # Output: 'dog' and 'cat' are semantically similar (similarity: 0.86)\n",
        "print(simple_semantic_reasoning(\"house\", \"flat\")) # Output: 'dog' and 'car' are not very similar (similarity: 0.17)\n",
        "print(simple_semantic_reasoning(\"chips\", \"potato\")) #Output: 'apple' and 'orange' are semantically similar (similarity: 0.83)\n",
        "print(simple_semantic_reasoning(\"chips\", \"poker\")) #Output: 'random' and 'word' are not very similar (similarity: 0.2)"
      ],
      "metadata": {
        "id": "n2t79RfJ4WSd",
        "outputId": "63ad81d6-f3f3-449f-8999-5528b62c9eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\n",
            "neg\n",
            "pos\n",
            "'house' and 'apartment' are semantically similar (similarity: 0.82)\n",
            "'house' and 'flat' are not very similar (similarity: 0.43)\n",
            "'chips' and 'potato' are semantically similar (similarity: 0.95)\n",
            "'chips' and 'poker' are not very similar (similarity: 0.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems with these easy examples:\n",
        "*Sentiment Analysis*\n",
        "- code works by simply checking if the words in the string are within the negative or positive word list; context is lost (e.g. sad doesnt always equate negative)\n",
        "- code doesn't handle negation (\"not good\")\n",
        "- sarcasm and irony are very hard for simple algorithms to detect\n",
        "- code doesn't look at the sentence as a whole\n",
        "\n",
        "*Semantic Reasoning*\n",
        "- \"house\" and \"flat\" have a lower similarity as they are not related in the way WordNets structure defines it; there are regional difference in the usage of the world \"flat\" (e.g. in the UK)\n",
        "- \"chips\" and \"poker\" have a low similarity because there are different meanings for the word \"chips\" (Potato or Poker Chips)\n",
        "\n",
        "-> Naive Bayes and WordNet-based similarity struggle with complexities of human language. Real-World NLP applications use way more sophisticated techniques such as deep learning models, more extensive lexical resources and techniques for handling negation, sarcasm and other linguistic phenomena."
      ],
      "metadata": {
        "id": "-832hos4-Q8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy"
      ],
      "metadata": {
        "id": "NotV-FuwALDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Python library for advanced NLP\n",
        "- designed for efficiency and speed, especially for large volume of text\n",
        "- provided pre-trained statistical models and word vactors for various languages\n",
        "- tools for core NLP tasks:\n",
        "  1. Tokenization (splitting text into words)\n",
        "  2. Lemmatization (reducing words to their base form, more accurate than stemming)\n",
        "  3. Part-of-Speech Tagging (assigning grammatical labels, e.g., nouns, verbs)\n",
        "  4. Named Entity Recognition (identifying and classifying entities, e.g., people, locations, organizations)\n",
        "  5. Dependency Parsing (analyzing sentence structure and relationships between words)\n",
        "  6. Text Classification (using external libraries like TextBlob for simple tasks)\n",
        "  7. Semantic Similarity (computing similarity between words and documents using word vectors)\n",
        "- free and open-source and cross-platform compatibility (Windows, macOS, Linux)\n",
        "\n",
        "## Resources:\n",
        "- https://spacy.io/"
      ],
      "metadata": {
        "id": "KJg-jyCYAsCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load the English language model\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "text = \"At the Broad Institute in Cambridge, Dr. Jennifer Doudna's work with CRISPR technology transformed genetic research.\"\n",
        "\n",
        "# 1. Tokenization and POS Tagging (Combined in spaCy)\n",
        "doc = nlp(text)\n",
        "token_pos = [(token.text, token.pos_) for token in doc]\n",
        "print(\"\\n--- Tokenization and Part-of-Speech Tagging ---\")\n",
        "print(tabulate(token_pos, headers=[\"Token\", \"POS Tag\"], tablefmt=\"fancy_grid\"))\n",
        "\n",
        "# 2. Stemming (spaCy uses lemmatization, which is more accurate)\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "print(\"\\n--- Lemmatization ---\")\n",
        "print(lemmatized_tokens)\n",
        "\n",
        "# 4. Named Entity Recognition (NER)\n",
        "ner_results = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\n--- Named Entity Recognition ---\")\n",
        "print(tabulate(ner_results, headers=[\"Entity\", \"NER Tag\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "WuanCkCWANPH",
        "outputId": "0e883efd-0206-4177-c6c0-7ae539d953ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "--- Tokenization and Part-of-Speech Tagging ---\n",
            "╒═════════════╤═══════════╕\n",
            "│ Token       │ POS Tag   │\n",
            "╞═════════════╪═══════════╡\n",
            "│ At          │ ADP       │\n",
            "├─────────────┼───────────┤\n",
            "│ the         │ DET       │\n",
            "├─────────────┼───────────┤\n",
            "│ Broad       │ PROPN     │\n",
            "├─────────────┼───────────┤\n",
            "│ Institute   │ PROPN     │\n",
            "├─────────────┼───────────┤\n",
            "│ in          │ ADP       │\n",
            "├─────────────┼───────────┤\n",
            "│ Cambridge   │ PROPN     │\n",
            "├─────────────┼───────────┤\n",
            "│ ,           │ PUNCT     │\n",
            "├─────────────┼───────────┤\n",
            "│ Dr.         │ PROPN     │\n",
            "├─────────────┼───────────┤\n",
            "│ Jennifer    │ PROPN     │\n",
            "├─────────────┼───────────┤\n",
            "│ Doudna      │ PROPN     │\n",
            "├─────────────┼───────────┤\n",
            "│ 's          │ PART      │\n",
            "├─────────────┼───────────┤\n",
            "│ work        │ NOUN      │\n",
            "├─────────────┼───────────┤\n",
            "│ with        │ ADP       │\n",
            "├─────────────┼───────────┤\n",
            "│ CRISPR      │ PROPN     │\n",
            "├─────────────┼───────────┤\n",
            "│ technology  │ NOUN      │\n",
            "├─────────────┼───────────┤\n",
            "│ transformed │ VERB      │\n",
            "├─────────────┼───────────┤\n",
            "│ genetic     │ ADJ       │\n",
            "├─────────────┼───────────┤\n",
            "│ research    │ NOUN      │\n",
            "├─────────────┼───────────┤\n",
            "│ .           │ PUNCT     │\n",
            "╘═════════════╧═══════════╛\n",
            "\n",
            "--- Lemmatization ---\n",
            "['at', 'the', 'Broad', 'Institute', 'in', 'Cambridge', ',', 'Dr.', 'Jennifer', 'Doudna', \"'s\", 'work', 'with', 'CRISPR', 'technology', 'transform', 'genetic', 'research', '.']\n",
            "\n",
            "--- Named Entity Recognition ---\n",
            "╒═════════════════════╤═══════════╕\n",
            "│ Entity              │ NER Tag   │\n",
            "╞═════════════════════╪═══════════╡\n",
            "│ the Broad Institute │ ORG       │\n",
            "├─────────────────────┼───────────┤\n",
            "│ Cambridge           │ GPE       │\n",
            "├─────────────────────┼───────────┤\n",
            "│ Jennifer Doudna's   │ PERSON    │\n",
            "├─────────────────────┼───────────┤\n",
            "│ CRISPR              │ ORG       │\n",
            "╘═════════════════════╧═══════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Sentiment Analysis (using TextBlob for simplicity)\n",
        "from textblob import TextBlob\n",
        "\n",
        "def simple_sentiment_analysis_textblob(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return \"pos\"\n",
        "    elif sentiment < 0:\n",
        "        return \"neg\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "print(\"\\n--- Sentiment Analysis (TextBlob) ---\")\n",
        "print(simple_sentiment_analysis_textblob(\"I really liked the play, the actors were amazing!\"))\n",
        "print(simple_sentiment_analysis_textblob(\"This play was so sad. I felt awful afterwards.\"))\n",
        "print(simple_sentiment_analysis_textblob(\"This play was so sad. I felt awful afterwards. The acting was fantastic and the sound quality good.\"))\n",
        "\n",
        "# Example Semantic Reasoning (using spaCy similarity)\n",
        "def simple_semantic_reasoning_spacy(word1, word2):\n",
        "    token1 = nlp(word1)\n",
        "    token2 = nlp(word2)\n",
        "    similarity = token1.similarity(token2)\n",
        "    return f\"'{word1}' and '{word2}' similarity: {similarity:.2f}\"\n",
        "\n",
        "print(\"\\n--- Semantic Reasoning (spaCy) ---\")\n",
        "print(simple_semantic_reasoning_spacy(\"house\", \"apartment\"))\n",
        "print(simple_semantic_reasoning_spacy(\"house\", \"flat\"))\n",
        "print(simple_semantic_reasoning_spacy(\"chips\", \"potato\"))\n",
        "print(simple_semantic_reasoning_spacy(\"fries\", \"potato\"))\n",
        "print(simple_semantic_reasoning_spacy(\"chips\", \"poker\"))"
      ],
      "metadata": {
        "id": "UTNHMdp7BWtO",
        "outputId": "cf002ba5-580b-4fda-9e42-edb6a544aed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentiment Analysis (TextBlob) ---\n",
            "pos\n",
            "neg\n",
            "neg\n",
            "\n",
            "--- Semantic Reasoning (spaCy) ---\n",
            "'house' and 'apartment' similarity: 0.33\n",
            "'house' and 'flat' similarity: 0.33\n",
            "'chips' and 'potato' similarity: 0.16\n",
            "'fries' and 'potato' similarity: 1.00\n",
            "'chips' and 'poker' similarity: 0.03\n",
            "'draw' and 'poker' similarity: 0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems with these easy examples:\n",
        "*Sentiment Analysis (TextBlob)*\n",
        "- Relies on simple polarity scores, losing context\n",
        "- Doesn't handle negation well\n",
        "- Struggles with sarcasm and complex language\n",
        "- Doesn't understand the whole sentence's semantic structure\n",
        "\n",
        "*Semantic Reasoning (spaCy)*\n",
        "- Similarity is based on vector representations, which can be context-dependent\n",
        "- 'Chips' ambiguity is still problematic\n",
        "- Doesn't fully understand nuanced word relationships\n",
        "- Similarity is based on statistical co-occurrence, not deep semantic understanding\n"
      ],
      "metadata": {
        "id": "mkMEQ3OrCS9V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-Ef8dAADSNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Willkommen bei Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}